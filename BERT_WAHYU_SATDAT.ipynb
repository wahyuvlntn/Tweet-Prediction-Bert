{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wahyuvlntn/Tweet-Prediction-Bert/blob/main/BERT_WAHYU_SATDAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jduhFvhzzzi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f238aa-9099-4f7e-f01f-077373865fcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.37.2 in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.37.2) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.37.2) (2024.6.2)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.1)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n",
            "Requirement already satisfied: tf-keras==2.15.1 in /usr/local/lib/python3.10/dist-packages (2.15.1)\n",
            "Requirement already satisfied: tensorflow<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tf-keras==2.15.1) (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16,>=2.15->tf-keras==2.15.1) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.15->tf-keras==2.15.1) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras==2.15.1) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras==2.15.1) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras==2.15.1) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras==2.15.1) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras==2.15.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras==2.15.1) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras==2.15.1) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras==2.15.1) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras==2.15.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras==2.15.1) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras==2.15.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras==2.15.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras==2.15.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras==2.15.1) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras==2.15.1) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras==2.15.1) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.15->tf-keras==2.15.1) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.37.2\n",
        "!pip install tensorflow-addons\n",
        "!pip install tf-keras==2.15.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2EqCH8wVneZ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install numpy requests nlpaug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDNWCN2wF49v",
        "outputId": "bd385dae-a8ed-414b-9cdb-15b7d1749ea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sm0awV6WFUd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxdDBy-7Rf3L"
      },
      "source": [
        "ini cuma contoh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR6nFZJ6QjCm"
      },
      "outputs": [],
      "source": [
        "contoh = {\n",
        "    'text': [\n",
        "        \"@CIqXqwGAT04tMtx4OCATxjoVq7vv/Y8HeYaIOgMFg8Y= emng bener sih, pendukung 01 ada yg goblok, begitu jg dg pendukung 02.. hnya sj menurut pak Ridwan Kamil skemanya terbalik, klo 01 MAYORITAS PENDIDIKAN MENENGAH ATAS (artinya ada jg pendidikan rendah yg milih\",\n",
        "        \"RT @L3R8XFBw3WGbxRPSj0/0hHZTbqVGX7qtfwRg9zmhK7Q= Sewaktu anies bersikap kritis ke kinerja pak prabowo dianggap engga sopan karena dianggap kurang menghormati orang tua, giliran skrg gibran yg tengil dan sok kritis malah dianggap kritis dan keras. Apakah ini tidak standar ganda? [RE coldthem]\",\n",
        "        \"RT Duh jangan sampai Pak lurah denger nih ÃƒÂ°Ã‚Å¸Ã‚Â¤Ã‚Â­ Di Acara Hajatan Rakyat, Puluhan Ribu Warga di Kendal Serukan ÃƒÂ¢Ã‚â‚¬Ã‚Å“Ganjar PresidenÃƒÂ¢Ã‚â‚¬Ã‚Â Kehadiran @0Zdeh9QcTWu+z+fS3hRaTcFuSLRh56REFyRLq4/Jdlc= menjadi magnet bagi puluhan ribu warga untuk datang menghadiri Hajatan Rakyat Ganjar-Mahfud. Besarnya antusiasme warga menjadi bukti bahwa Jawa Tengah tetap menjadi kandang banteng. #GanjarMahfud2024 #TabrakProfMahfud [RE DS_yantie]\"\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR1zbnOSBUfM"
      },
      "outputs": [],
      "source": [
        "X_train = pd.read_csv('final_train_2.csv')\n",
        "X_test = pd.read_csv('final_test_2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYttxmkucSTT"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.sample(frac=1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXEs6Cm6dhcQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "X_train['processed_text'] = X_train['processed_text'].astype(str)\n",
        "X_test['processed_text'] = X_test['processed_text'].astype(str)\n",
        "\n",
        "X_train['label'] = X_train['label'].astype(str)\n",
        "X_test['label'] = X_test['label'].astype(str)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "X_train['label'] = label_encoder.fit_transform(X_train['label'])\n",
        "X_test['label'] = label_encoder.fit_transform(X_test['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10oyEZJn3_iT"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel, TFAutoModel, TFAutoModelForSequenceClassification, create_optimizer, AutoConfig, AdamWeightDecay\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYwFp-Z6Db42"
      },
      "outputs": [],
      "source": [
        "def clean(text):\n",
        "    text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[#]+|[^A-Za-z0-9]+\"\n",
        "    text = re.sub(text_cleaning_re, \" \", text).strip()\n",
        "    text = text.strip()\n",
        "\n",
        "    out = []\n",
        "    for word in text.split():\n",
        "        out.append(word)\n",
        "\n",
        "    return \" \".join(out).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCTgCk7oFI6f"
      },
      "outputs": [],
      "source": [
        "X_train[\"processed_text\"] = X_train[\"processed_text\"].apply(clean)\n",
        "X_test[\"processed_text\"] = X_test[\"processed_text\"].apply(clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFgRlBjK-SRz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "34c624e7-6f2d-4d0a-c2e1-4c1bb3c14e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertModel.\n",
            "\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-large-p1\")\n",
        "IndoBert = TFAutoModel.from_pretrained(\"indobenchmark/indobert-large-p1\", from_pt=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ahcbb6O6B_Le"
      },
      "outputs": [],
      "source": [
        "tokens_train = tokenizer(\n",
        "    X_train[\"processed_text\"].tolist(),\n",
        "    max_length=64,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"tf\"\n",
        ")\n",
        "\n",
        "tokens_val = tokenizer(\n",
        "    X_test[\"processed_text\"].tolist(),\n",
        "    max_length=64,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"tf\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_wBKY8jClEi"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train = to_categorical(X_train.label.tolist(), 8)  # Assuming there are 8 classes\n",
        "y_val = to_categorical(X_test.label.tolist(), 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xZdNu_j-UFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12593ea4-f5b3-44ea-f581-5f3ebdc780cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5135, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ADASYN\n",
        "\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Extract input_ids\n",
        "input_ids_train = tokens_train[\"input_ids\"]\n",
        "\n",
        "# Apply SMOTE to balance the classes\n",
        "adasyn = ADASYN(random_state=42)\n",
        "input_ids_train_resampled, y_train_resampled = adasyn.fit_resample(input_ids_train, X_train['label'].tolist())\n",
        "\n",
        "# Generate attention masks for resampled data\n",
        "attention_mask_resampled = np.where(input_ids_train_resampled != tokenizer.pad_token_id, 1, 0)\n",
        "\n",
        "# Convert labels to categorical format\n",
        "y_train_resampled_categorical = to_categorical(y_train_resampled, 8)  # Assuming there are 8 classes\n",
        "y_val = to_categorical(X_test['label'].tolist(), 8)\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "train_data_resampled = tf.data.Dataset.from_tensor_slices((\n",
        "    {\"input_ids\": input_ids_train_resampled, \"attention_mask\": attention_mask_resampled},\n",
        "    y_train_resampled_categorical\n",
        ")).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "validation_sequence = tf.data.Dataset.from_tensor_slices((\n",
        "    {\"input_ids\": tokens_val[\"input_ids\"], \"attention_mask\": tokens_val[\"attention_mask\"]},\n",
        "    y_val\n",
        ")).batch(32).prefetch(tf.data.experimental.AUTOTUNE).cache()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4rSobuKZCRR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title SMOTE\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Extract input_ids\n",
        "input_ids_train = tokens_train[\"input_ids\"]\n",
        "\n",
        "# Apply SMOTE to balance the classes\n",
        "smote = SMOTE(random_state=42)\n",
        "input_ids_train_resampled, y_train_resampled = smote.fit_resample(input_ids_train, X_train['label'].tolist())\n",
        "\n",
        "# Generate attention masks for resampled data\n",
        "attention_mask_resampled = np.where(input_ids_train_resampled != tokenizer.pad_token_id, 1, 0)\n",
        "\n",
        "# Convert labels to categorical format\n",
        "y_train_resampled_categorical = to_categorical(y_train_resampled, 8)  # Assuming there are 8 classes\n",
        "y_val = to_categorical(X_test['label'].tolist(), 8)\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "train_data_resampled = tf.data.Dataset.from_tensor_slices((\n",
        "    {\"input_ids\": input_ids_train_resampled, \"attention_mask\": attention_mask_resampled},\n",
        "    y_train_resampled_categorical\n",
        ")).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "validation_sequence = tf.data.Dataset.from_tensor_slices((\n",
        "    {\"input_ids\": tokens_val[\"input_ids\"], \"attention_mask\": tokens_val[\"attention_mask\"]},\n",
        "    y_val\n",
        ")).batch(32).prefetch(tf.data.experimental.AUTOTUNE).cache()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "C0sWSWjf6hgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0jZZkv1I_cb"
      },
      "outputs": [],
      "source": [
        "train_data = tf.data.Dataset.from_tensor_slices((\n",
        "    {\"input_ids\": tokens_train[\"input_ids\"], \"attention_mask\": tokens_train[\"attention_mask\"]},\n",
        "    y_train\n",
        ")).batch(32).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "validation_sequence = tf.data.Dataset.from_tensor_slices((\n",
        "    {\"input_ids\": tokens_val[\"input_ids\"], \"attention_mask\": tokens_val[\"attention_mask\"]},\n",
        "    y_val\n",
        ")).batch(32).prefetch(tf.data.experimental.AUTOTUNE).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPXzGjMx_gFz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f069572b-bdbd-4fa7-a637-a944295afeda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 2.5270669291338583, 1: 2.090798045602606, 2: 4.114583333333333, 3: 1.3099489795918366, 4: 1.3744646680942185, 5: 0.2691299790356394, 6: 0.8964734636871509, 7: 1.7829861111111112}\n",
            "Class 0: 254\n",
            "Class 1: 307\n",
            "Class 2: 156\n",
            "Class 3: 490\n",
            "Class 4: 467\n",
            "Class 5: 2385\n",
            "Class 6: 716\n",
            "Class 7: 360\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# classweights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(X_train.label.tolist()), y=X_train.label.tolist())\n",
        "class_weights_dict = {i : class_weights[i] for i in range(len(class_weights))}\n",
        "print(class_weights_dict)\n",
        "\n",
        "#print frekuensi\n",
        "unique, counts = np.unique(X_train.label.tolist(), return_counts=True)\n",
        "for class_label, frequency in zip(unique, counts):\n",
        "    print(f'Class {class_label}: {frequency}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us5MkuAYcmKE"
      },
      "outputs": [],
      "source": [
        "seed_value = 42\n",
        "np.random.seed(seed_value)\n",
        "tf.random.set_seed(seed_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frieFT9UzlHG",
        "outputId": "139460a4-e7d1-43fc-b354-0ff34be96bd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Demografi' 'Ekonomi' 'Geografi' 'Ideologi' 'Pertahanan dan Keamanan'\n",
            " 'Politik' 'Sosial Budaya' 'Sumber Daya Alam']\n"
          ]
        }
      ],
      "source": [
        "print(label_encoder.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HEfivAALzEGs"
      },
      "outputs": [],
      "source": [
        "# @title LDAM Loss\n",
        "\n",
        "class LDAMLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, cls_num_list, max_m=0.5, s=30):\n",
        "        super(LDAMLoss, self).__init__()\n",
        "        m_list = 1.0 / np.sqrt(np.sqrt(cls_num_list))\n",
        "        m_list = m_list * (max_m / np.max(m_list))\n",
        "        self.m_list = tf.convert_to_tensor(m_list, dtype=tf.float32)\n",
        "        self.s = s\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        # Ensure y_true and y_pred have the same shape\n",
        "        if y_true.shape != y_pred.shape:\n",
        "            raise ValueError(\"Shapes of y_true and y_pred must match\")\n",
        "\n",
        "        # Calculate margins for each class based on the true labels\n",
        "        margins = tf.reduce_sum(y_true * self.m_list, axis=-1, keepdims=True)\n",
        "\n",
        "        # Apply the margins to the predictions\n",
        "        y_pred_m = y_pred - margins\n",
        "\n",
        "        # Scale the logits\n",
        "        y_pred_scaled = self.s * y_pred_m\n",
        "\n",
        "        # Calculate the loss\n",
        "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred_scaled))\n",
        "\n",
        "\n",
        "# import tensorflow_model_analysis as tfma\n",
        "num_classes = 8  # Number of classes in your dataset\n",
        "cls_num_list = [49, 308, 16, 319, 304, 2386, 467, 151]\n",
        "\n",
        "# Create a custom model architecture using IndoBert as a base\n",
        "input_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
        "attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
        "\n",
        "embeddings = IndoBert(input_ids, attention_mask=attention_mask)[0]\n",
        "pooled_output = tf.keras.layers.GlobalAveragePooling1D()(embeddings)\n",
        "dropout = tf.keras.layers.Dropout(0.1)(pooled_output)\n",
        "outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(dropout)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "              loss=LDAMLoss(cls_num_list=cls_num_list),\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VpkFT3yHHNAb"
      },
      "outputs": [],
      "source": [
        "# @title Simple\n",
        "num_classes = 8\n",
        "\n",
        "# for layer in IndoBert.layers:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# Create a custom model architecture using IndoBert as a base\n",
        "input_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
        "attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
        "\n",
        "bert_outputs = IndoBert(input_ids, attention_mask=attention_mask)\n",
        "pooled_output = bert_outputs.pooler_output\n",
        "bert_outputs.trainable = False\n",
        "\n",
        "dense_layer = tf.keras.layers.Dense(512, activation='relu')(pooled_output)\n",
        "dense_layer = tf.keras.layers.Dense(512, activation='relu')(pooled_output)\n",
        "dropout = tf.keras.layers.Dropout(0.2)(dense_layer)\n",
        "dense_layer = tf.keras.layers.Dense(1024, activation='relu')(dropout)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(dense_layer)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=outputs)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cdbFL9MQwBZd"
      },
      "outputs": [],
      "source": [
        "# @title Simple 2 Use last_hidden_state\n",
        "num_classes = 8\n",
        "\n",
        "for layer in IndoBert.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a custom model architecture using IndoBert as a base\n",
        "input_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
        "attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
        "\n",
        "bert_outputs = IndoBert(input_ids, attention_mask=attention_mask)\n",
        "last_hidden_state = bert_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "\"\"\"\n",
        "use global average pooling\n",
        "last_hidden_state = bert_outputs.last_hidden_state\n",
        "avg_pool = tf.keras.layers.GlobalAveragePooling1D()(last_hidden_state)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# dropout = tf.keras.layers.Dropout(0.2)(last_hidden_state)\n",
        "dense_layer = tf.keras.layers.Dense(512, activation='relu')(last_hidden_state)\n",
        "\n",
        "outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(dense_layer)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=outputs)\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n00r98gIezxj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "0058f937e6bb4036a0ea0966493e9df9",
            "ce47413d0c0d48c1a7f2f684537141f0",
            "e9d44197edd243ef8347b7a4155e746d",
            "9728fbbe4381424f844fc58f3a9e3300",
            "7e58b30924ff4cf0a84843658747b2c4",
            "116afa98800d47b6ac34230a2a049191",
            "1888873906d7490aa6ed550bf63db59d",
            "8c5d795f54524de98b940b07277c429f",
            "497bd187bc994f24bc9363cdf55da80f",
            "54d376ed9c06439cb8f6d2c4d91cb3ea",
            "9b5f190d12a94be085953c89c5d93337"
          ]
        },
        "outputId": "37489a48-f20d-488c-8673-86be904e73d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tf_model.h5:   0%|          | 0.00/1.47G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0058f937e6bb4036a0ea0966493e9df9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at indobenchmark/indobert-large-p1 were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at indobenchmark/indobert-large-p1.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# # @title LSTM model\n",
        "from transformers import TFBertModel\n",
        "class BERTLSTMClassifier(tf.keras.Model):\n",
        "    def __init__(self, num_classes, hidden_size):\n",
        "        super(BERTLSTMClassifier, self).__init__()\n",
        "        self.bert = TFBertModel.from_pretrained('indobenchmark/indobert-large-p1')\n",
        "        self.lstm = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.dense1 = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
        "        self.batch_norm = tf.keras.layers.BatchNormalization()\n",
        "        self.dense2 = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_ids, attention_mask = inputs\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        lstm_output = self.lstm(outputs.last_hidden_state)\n",
        "        # mean_pooling = tf.reduce_mean(lstm_output, axis=1)  # Mean pooling\n",
        "        max_pooling = tf.reduce_max(lstm_output, axis=1)  # max pooling\n",
        "        x = self.dense1(max_pooling)\n",
        "        x = self.batch_norm(x)\n",
        "        logits = self.dense2(x)\n",
        "        return logits\n",
        "\n",
        "input_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name='attention_mask')\n",
        "outputs = BERTLSTMClassifier(num_classes=8, hidden_size=768)((input_ids, attention_mask))\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=outputs)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNcAWUOyqUYb",
        "outputId": "efaea9c4-410c-4e66-ef07-9ac22cca3651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some layers from the model checkpoint at indobenchmark/indobert-large-p1 were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at indobenchmark/indobert-large-p1.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFBertModel\n",
        "\n",
        "class BERTMeanPoolingClassifier(tf.keras.Model):\n",
        "    def __init__(self, num_classes, hidden_size):\n",
        "        super(BERTMeanPoolingClassifier, self).__init__()\n",
        "        self.bert = TFBertModel.from_pretrained('indobenchmark/indobert-large-p1')\n",
        "        self.dense1 = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
        "        # self.dropout = tf.keras.layers.Dropout(0.3)\n",
        "        self.dense12 = tf.keras.layers.Dense(hidden_size // 2, activation='relu')\n",
        "        self.batch_norm = tf.keras.layers.BatchNormalization()\n",
        "        self.dense2 = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_ids, attention_mask = inputs\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        mean_pooling = tf.reduce_mean(outputs.last_hidden_state, axis=1)  # Mean pooling\n",
        "        x = self.dense1(mean_pooling)\n",
        "        # x = self.dropout(x)\n",
        "        x = self.dense12(x)\n",
        "        x = self.batch_norm(x)\n",
        "        logits = self.dense2(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "input_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name='input_ids')\n",
        "attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name='attention_mask')\n",
        "outputs = BERTMeanPoolingClassifier(num_classes=8, hidden_size=768)((input_ids, attention_mask))\n",
        "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=outputs)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCeI6mqyr-Sg",
        "outputId": "006adca7-fc97-4979-9c0c-0706119f012f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "161/161 [==============================] - 103s 643ms/step - loss: 0.0708 - accuracy: 0.9673 - val_loss: 0.9868 - val_accuracy: 0.7490\n",
            "Epoch 2/3\n",
            "161/161 [==============================] - 103s 643ms/step - loss: 0.0643 - accuracy: 0.9696 - val_loss: 1.0025 - val_accuracy: 0.7420\n",
            "Epoch 3/3\n",
            "161/161 [==============================] - 103s 642ms/step - loss: 0.0599 - accuracy: 0.9722 - val_loss: 1.0259 - val_accuracy: 0.7420\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a2ca8b7ca90>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "class LearningRateScheduler(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, initial_lr, factor, epochs_per_decay):\n",
        "        super(LearningRateScheduler, self).__init__()\n",
        "        self.initial_lr = initial_lr\n",
        "        self.factor = factor\n",
        "        self.epochs_per_decay = epochs_per_decay\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        if (epoch + 1) % self.epochs_per_decay == 0:\n",
        "            lr = self.initial_lr * (self.factor ** ((epoch + 1) // self.epochs_per_decay))\n",
        "            tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
        "            print(f'Learning rate adjusted to {lr} for epoch {epoch + 1}')\n",
        "\n",
        "callbacks = [\n",
        "    # tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3),\n",
        "    LearningRateScheduler(initial_lr=2e-5, factor=0.1, epochs_per_decay=5)\n",
        "    # tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=2, min_lr=1e-8)\n",
        "]\n",
        "\n",
        "model.fit(\n",
        "    train_data,\n",
        "    validation_data=validation_sequence,\n",
        "    epochs=3,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights_dict,\n",
        "    callbacks=callbacks\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HASIL BIASA"
      ],
      "metadata": {
        "id": "wGGmlrOCBS75"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4bm1YgLocW4",
        "outputId": "cf811531-5174-4b32-a180-c61b9afc6a41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "161/161 [==============================] - 29s 183ms/step\n",
            "Training - Predicted values:\n",
            "[3 5 5 ... 5 4 5]\n",
            "Training - Actual values:\n",
            "[3 5 5 ... 5 4 5]\n",
            "32/32 [==============================] - 6s 177ms/step\n",
            "Validation - Predicted values:\n",
            "[5 4 4 5 5 6 5 5 5 6 1 3 6 5 6 7 5 5 4 3 0 4 5 1 5 3 4 4 5 1 6 4 6 5 5 5 5\n",
            " 5 5 1 1 5 5 5 5 3 6 5 5 5 5 3 5 5 3 1 6 5 5 3 4 7 6 4 5 4 5 4 5 5 3 5 4 1\n",
            " 7 5 5 5 5 6 1 5 5 1 6 5 5 5 5 6 3 5 0 6 4 5 5 6 4 5 6 5 5 5 5 4 3 5 5 6 5\n",
            " 6 5 5 0 5 5 0 6 0 0 5 5 6 7 5 5 5 5 4 6 5 6 5 6 4 3 5 5 5 5 5 4 5 5 5 5 1\n",
            " 0 5 3 6 5 4 7 6 1 6 5 5 5 5 5 1 4 4 5 6 3 5 6 4 4 6 3 5 5 5 5 4 5 5 5 5 1\n",
            " 5 5 6 5 5 4 5 6 5 6 4 5 5 5 6 2 3 5 4 5 5 5 5 1 4 5 5 3 6 6 0 5 5 5 5 5 6\n",
            " 6 5 5 1 5 5 5 5 4 5 5 5 5 5 6 5 5 3 5 5 3 3 5 7 7 3 4 5 4 3 4 5 5 4 5 5 1\n",
            " 6 5 5 5 1 5 1 4 5 5 5 5 4 6 4 5 5 4 1 5 5 5 3 5 3 3 3 5 3 5 5 5 5 6 5 5 5\n",
            " 5 7 5 5 6 5 6 6 5 5 3 5 5 5 5 5 6 5 5 5 5 4 6 5 5 5 0 3 5 5 5 4 5 5 5 6 5\n",
            " 7 5 7 5 7 6 6 4 4 5 3 3 5 5 5 5 5 5 4 5 3 4 5 5 4 1 5 5 5 5 3 5 3 5 5 7 5\n",
            " 5 5 4 3 5 4 5 5 1 5 5 5 5 4 5 5 1 1 5 5 5 5 4 5 5 4 5 5 5 5 7 4 5 5 5 6 5\n",
            " 7 3 3 5 5 5 6 5 5 5 4 5 5 6 4 5 5 6 4 5 5 5 5 4 3 6 5 4 5 6 4 1 5 2 5 3 5\n",
            " 6 5 1 3 1 1 5 5 5 5 3 5 5 1 4 4 5 7 5 5 5 1 5 5 4 3 5 5 5 2 5 6 5 1 6 5 6\n",
            " 3 5 6 5 5 4 5 2 5 6 5 6 6 1 0 5 5 3 4 4 5 4 5 4 5 1 5 5 1 5 4 6 5 4 5 5 5\n",
            " 5 5 7 5 4 5 5 5 5 6 5 1 5 3 1 5 6 5 5 5 2 5 6 5 5 0 5 5 5 6 5 4 5 5 5 1 5\n",
            " 5 7 5 5 4 5 5 5 5 5 5 5 4 1 5 6 1 0 5 5 5 5 4 1 5 5 5 1 7 5 5 4 5 5 6 6 6\n",
            " 6 5 7 3 7 4 6 3 5 5 5 4 5 5 5 5 6 5 5 5 4 4 5 5 5 5 6 2 5 5 4 1 5 7 5 3 0\n",
            " 5 5 1 6 3 4 5 5 4 6 7 5 4 5 5 5 5 5 5 5 5 6 6 6 6 6 6 7 5 5 7 5 1 4 6 0 4\n",
            " 5 3 5 5 5 6 3 5 5 6 5 1 6 5 5 7 4 5 5 5 5 6 5 5 5 6 3 7 5 5 1 3 5 5 5 5 5\n",
            " 5 4 1 5 5 4 5 7 5 5 3 1 6 5 5 5 5 5 3 6 5 5 5 5 5 1 5 6 7 6 4 5 5 5 7 5 5\n",
            " 6 3 6 3 5 1 5 5 4 5 5 1 4 3 5 3 5 5 1 6 6 1 5 6 2 3 3 3 2 5 5 5 5 5 5 6 7\n",
            " 4 4 5 0 4 5 5 5 5 6 5 5 4 5 5 4 4 5 5 5 3 6 5 7 5 5 6 7 6 6 6 6 5 5 7 6 5\n",
            " 1 1 5 1 4 4 6 5 7 5 6 6 5 5 1 5 5 6 5 5 1 7 4 5 5 1 5 5 5 4 5 5 5 5 4 3 5\n",
            " 6 1 7 1 5 1 6 5 5 5 4 4 1 7 5 5 5 5 5 5 5 4 5 5 5 4 5 5 6 5 5 7 5 6 3 4 4\n",
            " 5 5 3 5 6 5 6 6 5 5 7 3 1 5 2 5 4 6 6 1 5 5 3 4 5 5 6 5 5 5 5 5 1 5 5 5 5\n",
            " 3 5 4 2 5 5 5 4 1 5 5 7 6 5 4 5 5 5 3 5 5 5 5 5 0 5 2 6 5 5 6 5 5 5 6 6 4\n",
            " 5 5 5 5 5 4 5 5 1 3 4 5 6 1 4 3 5 7 5 5 5 3 5 4 4 6 4 4 4 5 7 5 7 5 5 5 5\n",
            " 5]\n",
            "Validation - Actual values:\n",
            "[3 4 4 5 6 6 5 5 5 0 1 5 5 5 6 7 5 5 4 3 5 4 5 7 5 3 4 5 5 1 3 5 6 5 5 5 5\n",
            " 5 5 1 1 6 5 0 5 5 6 5 5 5 5 3 6 5 3 0 6 5 5 6 4 4 6 4 5 2 5 4 5 5 3 5 4 5\n",
            " 3 5 5 5 5 6 1 3 5 5 6 5 5 6 5 6 3 5 5 6 5 5 5 6 4 5 5 5 5 5 5 5 3 5 5 6 5\n",
            " 7 5 6 0 5 5 7 6 4 0 5 6 6 7 5 5 3 5 4 6 5 0 5 6 4 6 5 5 5 4 5 6 5 5 5 5 1\n",
            " 1 5 5 5 5 4 7 5 5 5 5 5 5 5 5 5 4 4 5 6 5 5 6 4 4 6 5 5 5 5 5 4 5 5 5 5 1\n",
            " 5 5 6 5 5 4 5 7 3 5 4 5 3 4 6 5 5 5 4 5 7 5 5 1 4 5 5 5 6 5 5 5 5 5 5 6 6\n",
            " 5 5 5 1 7 5 5 5 4 5 5 5 5 5 6 5 5 3 5 5 3 3 5 6 5 3 4 5 4 5 5 5 5 4 5 5 1\n",
            " 6 5 5 5 7 5 7 4 3 5 5 5 4 6 4 5 5 5 5 5 5 5 6 5 3 6 5 5 3 5 5 5 5 6 5 5 5\n",
            " 5 7 5 5 5 3 1 6 5 5 3 5 5 5 5 5 5 5 5 5 5 4 6 5 5 5 6 3 5 5 5 5 6 5 5 6 5\n",
            " 5 5 5 6 5 6 5 4 4 1 3 3 5 5 5 6 5 3 4 5 1 4 5 5 4 1 5 6 5 5 3 5 3 5 5 7 5\n",
            " 5 4 4 5 5 4 5 5 5 5 5 5 6 4 5 5 1 1 5 5 5 5 5 5 5 4 5 5 5 5 3 5 3 5 5 3 5\n",
            " 7 6 3 5 5 5 4 5 5 5 5 5 6 3 4 5 5 5 7 5 5 5 5 0 3 6 5 4 5 6 5 5 5 5 5 3 5\n",
            " 6 5 5 3 1 5 5 5 5 6 3 5 5 1 4 5 5 1 3 5 5 1 4 6 4 3 5 5 5 7 5 1 5 1 7 5 5\n",
            " 5 1 7 5 5 4 5 5 5 6 3 5 6 1 3 5 5 3 5 4 5 4 5 4 5 5 5 0 5 5 4 1 5 4 5 5 5\n",
            " 5 5 5 5 4 5 5 5 5 6 5 5 5 5 1 6 6 5 3 5 5 5 3 5 3 0 5 5 5 6 6 1 5 3 5 1 5\n",
            " 5 7 5 5 5 5 5 5 5 4 5 5 4 1 5 1 1 5 5 5 5 5 4 5 6 5 6 1 7 5 5 5 5 5 6 6 6\n",
            " 6 5 6 6 7 5 6 3 5 5 5 5 5 5 5 3 5 5 1 5 4 4 5 5 5 5 6 5 5 5 0 1 4 5 6 5 3\n",
            " 5 5 1 2 5 6 1 5 4 6 1 0 4 5 5 5 5 5 5 5 5 6 6 6 5 5 6 4 5 5 5 5 4 5 5 3 4\n",
            " 5 3 3 5 5 6 5 5 5 6 6 7 1 5 5 1 5 5 5 7 5 6 5 5 5 6 3 5 5 5 5 3 5 5 5 5 5\n",
            " 6 4 5 5 5 4 5 7 5 5 3 1 6 5 5 5 5 5 3 6 5 5 5 5 5 6 5 6 7 3 4 5 5 5 7 5 5\n",
            " 6 5 5 3 5 1 5 5 4 3 5 1 4 3 5 3 5 5 1 6 7 5 5 6 4 3 5 3 2 5 5 5 5 5 5 6 3\n",
            " 4 4 6 5 4 5 5 5 5 5 0 5 5 3 5 5 4 5 5 5 3 6 5 7 5 5 6 7 5 6 6 6 5 5 5 6 6\n",
            " 1 5 5 1 4 5 6 5 7 5 5 5 5 5 7 5 5 6 5 5 1 7 4 5 5 1 6 3 5 5 5 6 1 5 5 3 5\n",
            " 5 1 5 5 5 1 5 3 5 5 4 4 1 7 5 5 5 5 5 5 5 4 5 5 7 4 5 5 6 5 3 1 5 6 5 4 4\n",
            " 5 5 3 5 6 5 3 1 5 5 7 3 4 5 2 5 4 6 6 5 5 5 5 4 5 5 6 5 5 5 5 5 1 5 5 6 5\n",
            " 3 5 5 7 3 5 1 4 1 5 5 7 7 5 5 5 5 5 4 5 5 5 5 5 6 5 5 6 6 5 6 5 5 5 3 6 5\n",
            " 5 5 3 5 5 4 5 5 1 5 4 5 5 1 4 3 5 7 0 5 5 3 5 5 5 6 5 4 4 5 7 5 7 5 5 5 5\n",
            " 3]\n"
          ]
        }
      ],
      "source": [
        "# predict training data\n",
        "y_pred_train = model.predict(train_data)\n",
        "y_pred_train = np.argmax(y_pred_train, axis=1)\n",
        "\n",
        "# one-hot encoded y_train to original\n",
        "y_train_original = np.argmax(y_train, axis=1)\n",
        "\n",
        "# print\n",
        "print(\"Training - Predicted values:\")\n",
        "print(y_pred_train)\n",
        "\n",
        "print(\"Training - Actual values:\")\n",
        "print(y_train_original)\n",
        "\n",
        "# predict validation data\n",
        "y_pred_val = model.predict(validation_sequence)\n",
        "y_pred_val = np.argmax(y_pred_val, axis=1)\n",
        "\n",
        "# one-hot encoded y_val to original\n",
        "y_val_original = np.argmax(y_val, axis=1)\n",
        "\n",
        "# print\n",
        "print(\"Validation - Predicted values:\")\n",
        "print(y_pred_val)\n",
        "\n",
        "print(\"Validation - Actual values:\")\n",
        "print(y_val_original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VniI-TGM0Xu",
        "outputId": "3820830b-2d9d-419e-8f67-92510c3f4ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Balanced Accuracy: 0.9907094276844064\n",
            "Validation - Balanced Accuracy: 0.6003847945538725\n",
            "Training Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99       254\n",
            "           1       0.92      1.00      0.96       307\n",
            "           2       1.00      1.00      1.00       156\n",
            "           3       0.96      0.99      0.98       490\n",
            "           4       0.97      0.99      0.98       467\n",
            "           5       1.00      0.98      0.99      2385\n",
            "           6       0.99      0.98      0.98       716\n",
            "           7       0.98      0.99      0.98       360\n",
            "\n",
            "    accuracy                           0.98      5135\n",
            "   macro avg       0.98      0.99      0.98      5135\n",
            "weighted avg       0.98      0.98      0.98      5135\n",
            "\n",
            "Validation Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.19      0.23      0.21        13\n",
            "           1       0.59      0.68      0.63        59\n",
            "           2       0.18      0.50      0.27         4\n",
            "           3       0.61      0.54      0.58        81\n",
            "           4       0.69      0.85      0.76        96\n",
            "           5       0.87      0.80      0.83       586\n",
            "           6       0.62      0.66      0.64       120\n",
            "           7       0.51      0.54      0.52        41\n",
            "\n",
            "    accuracy                           0.74      1000\n",
            "   macro avg       0.53      0.60      0.55      1000\n",
            "weighted avg       0.76      0.74      0.75      1000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#[CLS] Token Representation\n",
        "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
        "\n",
        "# Calculate balanced accuracy for training data\n",
        "balanced_acc_train = balanced_accuracy_score(y_train_original, y_pred_train)\n",
        "\n",
        "# Calculate balanced accuracy for validation data\n",
        "balanced_acc_val = balanced_accuracy_score(y_val_original, y_pred_val)\n",
        "\n",
        "# Print the balanced accuracy\n",
        "print(\"Training - Balanced Accuracy:\", balanced_acc_train)\n",
        "print(\"Validation - Balanced Accuracy:\", balanced_acc_val)\n",
        "\n",
        "# Print classification report for training data\n",
        "print(\"Training Classification Report:\")\n",
        "print(classification_report(y_train_original, y_pred_train))\n",
        "\n",
        "# Print classification report for validation data\n",
        "print(\"Validation Classification Report:\")\n",
        "print(classification_report(y_val_original, y_pred_val))\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wvVUiJqS5j2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HASIL SMOTE"
      ],
      "metadata": {
        "id": "mGsToSckA4s8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict training data\n",
        "y_pred_train = model.predict(train_data)\n",
        "y_pred_train = np.argmax(y_pred_train, axis=1)\n",
        "\n",
        "# one-hot encoded y_train to original\n",
        "y_train_original = np.argmax(y_train, axis=1)\n",
        "\n",
        "# print\n",
        "print(\"Training - Predicted values:\")\n",
        "print(y_pred_train)\n",
        "\n",
        "print(\"Training - Actual values:\")\n",
        "print(y_train_original)\n",
        "\n",
        "# predict validation data\n",
        "y_pred_val = model.predict(validation_sequence)\n",
        "y_pred_val = np.argmax(y_pred_val, axis=1)\n",
        "\n",
        "# one-hot encoded y_val to original\n",
        "y_val_original = np.argmax(y_val, axis=1)\n",
        "\n",
        "# print\n",
        "print(\"Validation - Predicted values:\")\n",
        "print(y_pred_val)\n",
        "\n",
        "print(\"Validation - Actual values:\")\n",
        "print(y_val_original)"
      ],
      "metadata": {
        "id": "KXDHqSWWPI3V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86bb3795-e9cb-4c61-c5ca-75ef42bb82cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "161/161 [==============================] - 29s 178ms/step\n",
            "Training - Predicted values:\n",
            "[3 5 5 ... 5 4 5]\n",
            "Training - Actual values:\n",
            "[3 5 5 ... 5 4 5]\n",
            "32/32 [==============================] - 6s 174ms/step\n",
            "Validation - Predicted values:\n",
            "[5 4 4 5 5 6 5 5 5 6 1 5 1 5 6 7 5 5 4 3 5 4 5 1 5 3 4 4 5 1 3 4 6 5 5 5 5\n",
            " 5 5 1 1 5 5 6 5 3 6 5 5 5 5 3 5 5 3 1 6 5 5 3 4 6 6 4 5 6 5 4 5 5 3 5 4 1\n",
            " 7 5 5 5 5 6 1 5 5 4 6 5 5 5 5 6 3 5 5 6 4 5 5 6 4 5 5 5 5 3 5 4 3 5 5 6 1\n",
            " 6 5 5 0 5 5 0 6 4 0 5 6 6 7 5 5 5 5 4 6 5 6 5 6 4 3 5 5 5 2 5 6 5 5 5 6 1\n",
            " 1 5 3 6 5 4 7 6 1 6 5 5 5 5 5 6 4 4 5 6 3 5 6 4 4 6 6 5 5 5 5 4 5 5 5 5 1\n",
            " 0 5 6 5 4 4 5 6 5 5 4 5 4 6 6 5 3 4 4 5 5 5 5 1 4 5 5 5 6 6 0 5 5 4 5 5 6\n",
            " 4 5 5 6 5 5 5 5 4 5 5 4 6 5 6 5 5 3 5 5 3 3 5 5 6 3 4 5 4 6 4 5 5 4 1 5 1\n",
            " 6 4 5 6 1 6 7 4 5 5 5 5 4 6 4 5 5 4 0 5 5 5 6 5 4 6 5 5 3 5 5 5 5 6 5 5 5\n",
            " 5 7 5 5 6 5 6 6 5 5 3 5 5 5 5 5 1 5 5 5 5 4 6 5 5 5 6 3 5 5 5 4 6 5 5 6 5\n",
            " 5 6 7 6 3 6 5 4 4 5 3 6 5 5 6 5 5 3 4 5 5 4 5 4 4 1 5 5 3 5 3 5 3 5 5 7 5\n",
            " 5 4 4 5 5 4 5 5 1 6 5 5 5 4 5 5 1 1 1 5 5 4 6 5 5 4 5 5 5 5 7 4 5 5 5 5 5\n",
            " 7 6 3 5 5 5 6 5 5 5 5 5 5 6 4 5 3 6 4 5 5 5 5 4 3 6 5 4 5 6 2 5 5 0 5 3 5\n",
            " 6 5 1 3 1 1 5 5 5 6 3 5 5 1 4 4 5 7 5 5 5 1 4 5 4 3 5 5 5 4 5 5 5 1 6 5 6\n",
            " 5 5 4 5 5 4 5 1 5 4 5 6 6 1 5 3 6 3 5 4 5 4 5 4 5 1 5 0 1 5 4 6 5 4 5 5 5\n",
            " 5 5 7 5 4 5 5 5 5 6 5 1 5 3 1 6 6 5 1 5 5 5 6 3 5 0 5 5 5 6 2 4 1 5 5 1 5\n",
            " 5 7 5 5 4 5 5 5 5 5 5 5 4 1 5 6 1 5 5 5 5 5 4 4 5 5 5 1 7 5 5 4 5 5 6 6 6\n",
            " 6 5 6 7 7 4 6 3 5 5 5 4 5 5 5 5 6 6 1 5 4 4 5 5 5 5 6 6 5 5 6 1 4 4 5 5 0\n",
            " 5 5 1 6 3 4 5 5 4 6 1 5 4 0 5 5 5 5 5 5 5 6 6 6 5 5 6 5 6 5 7 5 4 4 6 4 4\n",
            " 5 3 4 5 5 6 3 5 5 6 5 1 6 5 5 7 4 5 5 6 5 6 5 5 5 6 3 5 5 5 1 3 5 4 5 5 5\n",
            " 6 4 1 5 5 4 5 7 5 5 6 1 6 5 5 5 5 5 3 6 5 5 5 5 5 1 5 6 7 6 4 5 5 5 2 5 5\n",
            " 6 3 6 3 5 1 5 5 4 5 5 1 4 3 5 3 5 5 1 6 6 1 5 6 6 3 5 3 2 5 1 5 5 5 5 6 7\n",
            " 4 4 5 5 4 5 5 5 5 6 5 5 6 5 5 5 4 5 5 5 3 6 4 7 5 5 6 7 6 6 6 6 5 5 5 5 6\n",
            " 1 1 4 1 4 4 6 5 1 5 6 1 5 0 1 5 5 6 6 5 1 7 4 5 5 1 3 5 5 4 5 5 5 5 4 5 5\n",
            " 6 1 7 5 5 1 6 5 5 5 4 4 1 7 5 5 5 5 5 5 5 4 5 5 5 4 5 5 6 5 5 1 5 6 5 4 4\n",
            " 5 5 3 5 6 5 7 6 5 5 7 3 5 5 6 4 4 6 6 5 5 5 6 4 6 5 6 5 5 1 5 5 1 5 5 5 5\n",
            " 3 5 5 4 5 5 5 4 1 1 5 1 5 5 4 6 4 5 5 5 5 5 6 5 6 5 1 6 5 5 6 5 5 5 3 6 5\n",
            " 5 5 5 5 5 4 5 5 1 5 4 5 6 5 4 0 5 7 0 5 5 3 5 4 4 6 4 1 4 5 7 5 7 5 5 5 5\n",
            " 5]\n",
            "Validation - Actual values:\n",
            "[3 4 4 5 6 6 5 5 5 0 1 5 5 5 6 7 5 5 4 3 5 4 5 7 5 3 4 5 5 1 3 5 6 5 5 5 5\n",
            " 5 5 1 1 6 5 0 5 5 6 5 5 5 5 3 6 5 3 0 6 5 5 6 4 4 6 4 5 2 5 4 5 5 3 5 4 5\n",
            " 3 5 5 5 5 6 1 3 5 5 6 5 5 6 5 6 3 5 5 6 5 5 5 6 4 5 5 5 5 5 5 5 3 5 5 6 5\n",
            " 7 5 6 0 5 5 7 6 4 0 5 6 6 7 5 5 3 5 4 6 5 0 5 6 4 6 5 5 5 4 5 6 5 5 5 5 1\n",
            " 1 5 5 5 5 4 7 5 5 5 5 5 5 5 5 5 4 4 5 6 5 5 6 4 4 6 5 5 5 5 5 4 5 5 5 5 1\n",
            " 5 5 6 5 5 4 5 7 3 5 4 5 3 4 6 5 5 5 4 5 7 5 5 1 4 5 5 5 6 5 5 5 5 5 5 6 6\n",
            " 5 5 5 1 7 5 5 5 4 5 5 5 5 5 6 5 5 3 5 5 3 3 5 6 5 3 4 5 4 5 5 5 5 4 5 5 1\n",
            " 6 5 5 5 7 5 7 4 3 5 5 5 4 6 4 5 5 5 5 5 5 5 6 5 3 6 5 5 3 5 5 5 5 6 5 5 5\n",
            " 5 7 5 5 5 3 1 6 5 5 3 5 5 5 5 5 5 5 5 5 5 4 6 5 5 5 6 3 5 5 5 5 6 5 5 6 5\n",
            " 5 5 5 6 5 6 5 4 4 1 3 3 5 5 5 6 5 3 4 5 1 4 5 5 4 1 5 6 5 5 3 5 3 5 5 7 5\n",
            " 5 4 4 5 5 4 5 5 5 5 5 5 6 4 5 5 1 1 5 5 5 5 5 5 5 4 5 5 5 5 3 5 3 5 5 3 5\n",
            " 7 6 3 5 5 5 4 5 5 5 5 5 6 3 4 5 5 5 7 5 5 5 5 0 3 6 5 4 5 6 5 5 5 5 5 3 5\n",
            " 6 5 5 3 1 5 5 5 5 6 3 5 5 1 4 5 5 1 3 5 5 1 4 6 4 3 5 5 5 7 5 1 5 1 7 5 5\n",
            " 5 1 7 5 5 4 5 5 5 6 3 5 6 1 3 5 5 3 5 4 5 4 5 4 5 5 5 0 5 5 4 1 5 4 5 5 5\n",
            " 5 5 5 5 4 5 5 5 5 6 5 5 5 5 1 6 6 5 3 5 5 5 3 5 3 0 5 5 5 6 6 1 5 3 5 1 5\n",
            " 5 7 5 5 5 5 5 5 5 4 5 5 4 1 5 1 1 5 5 5 5 5 4 5 6 5 6 1 7 5 5 5 5 5 6 6 6\n",
            " 6 5 6 6 7 5 6 3 5 5 5 5 5 5 5 3 5 5 1 5 4 4 5 5 5 5 6 5 5 5 0 1 4 5 6 5 3\n",
            " 5 5 1 2 5 6 1 5 4 6 1 0 4 5 5 5 5 5 5 5 5 6 6 6 5 5 6 4 5 5 5 5 4 5 5 3 4\n",
            " 5 3 3 5 5 6 5 5 5 6 6 7 1 5 5 1 5 5 5 7 5 6 5 5 5 6 3 5 5 5 5 3 5 5 5 5 5\n",
            " 6 4 5 5 5 4 5 7 5 5 3 1 6 5 5 5 5 5 3 6 5 5 5 5 5 6 5 6 7 3 4 5 5 5 7 5 5\n",
            " 6 5 5 3 5 1 5 5 4 3 5 1 4 3 5 3 5 5 1 6 7 5 5 6 4 3 5 3 2 5 5 5 5 5 5 6 3\n",
            " 4 4 6 5 4 5 5 5 5 5 0 5 5 3 5 5 4 5 5 5 3 6 5 7 5 5 6 7 5 6 6 6 5 5 5 6 6\n",
            " 1 5 5 1 4 5 6 5 7 5 5 5 5 5 7 5 5 6 5 5 1 7 4 5 5 1 6 3 5 5 5 6 1 5 5 3 5\n",
            " 5 1 5 5 5 1 5 3 5 5 4 4 1 7 5 5 5 5 5 5 5 4 5 5 7 4 5 5 6 5 3 1 5 6 5 4 4\n",
            " 5 5 3 5 6 5 3 1 5 5 7 3 4 5 2 5 4 6 6 5 5 5 5 4 5 5 6 5 5 5 5 5 1 5 5 6 5\n",
            " 3 5 5 7 3 5 1 4 1 5 5 7 7 5 5 5 5 5 4 5 5 5 5 5 6 5 5 6 6 5 6 5 5 5 3 6 5\n",
            " 5 5 3 5 5 4 5 5 1 5 4 5 5 1 4 3 5 7 0 5 5 3 5 5 5 6 5 4 4 5 7 5 7 5 5 5 5\n",
            " 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#[CLS] Token Representation\n",
        "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
        "\n",
        "# Calculate balanced accuracy for training data\n",
        "balanced_acc_train = balanced_accuracy_score(y_train_original, y_pred_train)\n",
        "\n",
        "# Calculate balanced accuracy for validation data\n",
        "balanced_acc_val = balanced_accuracy_score(y_val_original, y_pred_val)\n",
        "\n",
        "# Print the balanced accuracy\n",
        "print(\"Training - Balanced Accuracy:\", balanced_acc_train)\n",
        "print(\"Validation - Balanced Accuracy:\", balanced_acc_val)\n",
        "\n",
        "# Print classification report for training data\n",
        "print(\"Training Classification Report:\")\n",
        "print(classification_report(y_train_original, y_pred_train))\n",
        "\n",
        "# Print classification report for validation data\n",
        "print(\"Validation Classification Report:\")\n",
        "print(classification_report(y_val_original, y_pred_val))\n",
        "\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eL4ajuK5lhI",
        "outputId": "b2732c76-16c6-4d1e-deb0-3282fb4e7a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Balanced Accuracy: 0.9903553347606822\n",
            "Validation - Balanced Accuracy: 0.5989940987228244\n",
            "Training Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       254\n",
            "           1       0.93      0.99      0.96       307\n",
            "           2       0.99      1.00      1.00       156\n",
            "           3       0.99      0.99      0.99       490\n",
            "           4       0.95      0.99      0.97       467\n",
            "           5       1.00      0.98      0.99      2385\n",
            "           6       0.98      0.99      0.98       716\n",
            "           7       0.99      0.99      0.99       360\n",
            "\n",
            "    accuracy                           0.98      5135\n",
            "   macro avg       0.98      0.99      0.98      5135\n",
            "weighted avg       0.99      0.98      0.98      5135\n",
            "\n",
            "Validation Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.38      0.37        13\n",
            "           1       0.55      0.71      0.62        59\n",
            "           2       0.20      0.25      0.22         4\n",
            "           3       0.71      0.52      0.60        81\n",
            "           4       0.63      0.90      0.74        96\n",
            "           5       0.88      0.78      0.83       586\n",
            "           6       0.58      0.76      0.66       120\n",
            "           7       0.65      0.49      0.56        41\n",
            "\n",
            "    accuracy                           0.75      1000\n",
            "   macro avg       0.57      0.60      0.57      1000\n",
            "weighted avg       0.77      0.75      0.75      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sMuVf8uc5q4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HASIL ADASYN"
      ],
      "metadata": {
        "id": "JrBb7otPBHuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict training data\n",
        "y_pred_train = model.predict(train_data)\n",
        "y_pred_train = np.argmax(y_pred_train, axis=1)\n",
        "\n",
        "# one-hot encoded y_train to original\n",
        "y_train_original = np.argmax(y_train, axis=1)\n",
        "\n",
        "# print\n",
        "print(\"Training - Predicted values:\")\n",
        "print(y_pred_train)\n",
        "\n",
        "print(\"Training - Actual values:\")\n",
        "print(y_train_original)\n",
        "\n",
        "# predict validation data\n",
        "y_pred_val = model.predict(validation_sequence)\n",
        "y_pred_val = np.argmax(y_pred_val, axis=1)\n",
        "\n",
        "# one-hot encoded y_val to original\n",
        "y_val_original = np.argmax(y_val, axis=1)\n",
        "\n",
        "# print\n",
        "print(\"Validation - Predicted values:\")\n",
        "print(y_pred_val)\n",
        "\n",
        "print(\"Validation - Actual values:\")\n",
        "print(y_val_original)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-lwuEIyBDfv",
        "outputId": "72f0d97a-1741-4d51-fac2-d88e07e1ff6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "161/161 [==============================] - 34s 177ms/step\n",
            "Training - Predicted values:\n",
            "[3 6 5 ... 5 4 5]\n",
            "Training - Actual values:\n",
            "[3 5 5 ... 5 4 5]\n",
            "32/32 [==============================] - 6s 174ms/step\n",
            "Validation - Predicted values:\n",
            "[5 4 4 5 6 6 5 5 5 6 1 6 6 5 6 7 5 5 4 3 3 4 5 1 5 3 4 5 5 1 3 4 6 5 5 5 5\n",
            " 5 5 1 1 5 5 6 5 3 6 5 5 5 6 3 5 5 3 1 6 5 5 3 4 6 6 4 5 6 5 4 5 5 3 5 4 5\n",
            " 7 5 5 5 5 6 1 5 5 4 6 5 5 5 5 6 3 5 6 6 4 5 5 6 4 5 5 5 5 3 5 4 3 5 5 6 5\n",
            " 6 4 5 0 3 5 0 6 0 0 5 0 7 7 5 6 5 5 4 6 5 0 5 6 4 3 5 5 5 6 6 4 6 5 5 5 1\n",
            " 1 5 3 6 5 4 7 6 1 6 5 5 0 1 5 5 4 4 5 6 3 5 6 4 4 6 6 5 5 5 5 4 5 5 5 5 1\n",
            " 5 5 6 5 4 4 5 6 5 6 4 5 5 5 6 1 3 4 4 5 5 5 5 1 4 5 5 5 6 6 6 5 5 4 5 5 6\n",
            " 6 5 5 1 5 5 5 5 4 5 5 5 5 5 6 5 5 3 5 0 3 3 5 5 6 3 4 5 4 6 4 5 5 4 5 5 1\n",
            " 6 7 3 6 1 6 6 4 1 5 5 1 4 6 4 5 5 4 3 5 5 5 6 5 4 6 5 5 3 5 5 5 5 6 5 1 5\n",
            " 5 7 5 5 6 5 6 6 5 5 3 6 3 5 5 5 6 5 5 5 5 4 6 5 5 5 0 3 5 5 5 4 5 5 5 6 5\n",
            " 3 6 7 6 7 6 5 4 4 5 3 3 5 5 6 5 5 5 4 5 1 4 5 5 4 1 5 5 3 5 3 1 3 5 5 7 5\n",
            " 5 4 4 5 5 4 5 5 1 6 5 5 6 4 5 4 1 1 5 5 5 5 6 5 5 4 5 5 5 6 6 4 5 5 5 5 5\n",
            " 7 6 3 5 5 5 4 5 3 5 5 5 5 6 4 5 3 6 7 5 5 5 5 4 3 6 5 4 5 6 5 1 5 4 5 3 5\n",
            " 6 5 1 3 1 5 5 5 5 6 3 5 5 1 4 4 5 7 1 5 5 1 3 5 4 3 5 5 5 6 5 7 5 1 2 5 5\n",
            " 5 4 6 5 5 4 5 1 5 4 5 6 6 1 5 5 3 3 5 4 5 4 5 4 3 1 5 0 1 5 5 6 5 4 3 5 5\n",
            " 5 3 7 5 4 5 3 5 5 6 5 1 5 3 1 6 6 1 5 5 1 5 6 5 5 6 5 5 5 6 5 4 5 3 5 1 5\n",
            " 5 7 5 6 4 5 5 5 5 5 5 5 4 1 5 6 1 6 5 5 5 5 4 5 5 5 5 1 4 5 5 4 5 5 6 6 6\n",
            " 6 5 7 5 7 4 6 3 5 5 5 6 5 5 5 5 6 5 1 5 4 4 5 5 5 5 6 6 5 5 4 1 4 7 5 5 0\n",
            " 5 5 1 6 3 4 1 5 4 6 7 5 4 5 3 5 5 5 5 5 5 6 6 6 5 5 6 5 6 5 1 5 6 4 3 0 4\n",
            " 5 3 5 5 5 6 5 5 5 6 5 1 1 5 5 6 0 5 5 5 5 4 5 5 5 6 3 5 5 5 1 3 5 5 5 5 5\n",
            " 5 4 1 5 5 4 5 7 5 5 6 1 6 5 5 5 5 5 3 6 5 5 5 5 1 1 5 6 7 6 4 5 5 5 7 5 5\n",
            " 6 3 6 3 5 1 5 5 4 5 5 1 5 3 5 3 5 5 1 6 6 1 5 6 3 3 5 3 2 3 5 5 5 5 5 6 7\n",
            " 4 4 5 3 4 5 5 5 5 5 5 5 6 5 5 5 4 5 5 5 3 6 5 7 5 5 6 7 3 6 6 6 5 5 1 6 5\n",
            " 1 1 5 1 4 4 6 6 1 5 6 0 5 0 1 5 5 6 5 5 1 7 4 5 5 1 6 6 5 4 5 1 5 5 4 3 6\n",
            " 6 1 7 5 5 1 6 5 5 5 4 4 1 7 5 5 5 5 5 5 5 5 5 6 5 4 5 5 6 5 5 1 5 6 3 4 4\n",
            " 5 1 3 5 6 5 6 5 5 5 7 3 1 5 2 5 4 6 6 4 5 1 3 4 5 5 6 5 5 7 5 5 1 5 5 5 5\n",
            " 3 5 5 6 5 5 1 4 1 5 5 7 5 5 4 5 4 5 3 5 5 5 6 5 5 5 7 6 6 5 6 5 5 5 6 6 5\n",
            " 5 5 5 5 5 4 5 5 1 1 4 5 6 5 4 3 5 7 3 5 5 3 5 5 5 6 4 4 4 5 7 1 7 5 5 5 5\n",
            " 5]\n",
            "Validation - Actual values:\n",
            "[3 4 4 5 6 6 5 5 5 0 1 5 5 5 6 7 5 5 4 3 5 4 5 7 5 3 4 5 5 1 3 5 6 5 5 5 5\n",
            " 5 5 1 1 6 5 0 5 5 6 5 5 5 5 3 6 5 3 0 6 5 5 6 4 4 6 4 5 2 5 4 5 5 3 5 4 5\n",
            " 3 5 5 5 5 6 1 3 5 5 6 5 5 6 5 6 3 5 5 6 5 5 5 6 4 5 5 5 5 5 5 5 3 5 5 6 5\n",
            " 7 5 6 0 5 5 7 6 4 0 5 6 6 7 5 5 3 5 4 6 5 0 5 6 4 6 5 5 5 4 5 6 5 5 5 5 1\n",
            " 1 5 5 5 5 4 7 5 5 5 5 5 5 5 5 5 4 4 5 6 5 5 6 4 4 6 5 5 5 5 5 4 5 5 5 5 1\n",
            " 5 5 6 5 5 4 5 7 3 5 4 5 3 4 6 5 5 5 4 5 7 5 5 1 4 5 5 5 6 5 5 5 5 5 5 6 6\n",
            " 5 5 5 1 7 5 5 5 4 5 5 5 5 5 6 5 5 3 5 5 3 3 5 6 5 3 4 5 4 5 5 5 5 4 5 5 1\n",
            " 6 5 5 5 7 5 7 4 3 5 5 5 4 6 4 5 5 5 5 5 5 5 6 5 3 6 5 5 3 5 5 5 5 6 5 5 5\n",
            " 5 7 5 5 5 3 1 6 5 5 3 5 5 5 5 5 5 5 5 5 5 4 6 5 5 5 6 3 5 5 5 5 6 5 5 6 5\n",
            " 5 5 5 6 5 6 5 4 4 1 3 3 5 5 5 6 5 3 4 5 1 4 5 5 4 1 5 6 5 5 3 5 3 5 5 7 5\n",
            " 5 4 4 5 5 4 5 5 5 5 5 5 6 4 5 5 1 1 5 5 5 5 5 5 5 4 5 5 5 5 3 5 3 5 5 3 5\n",
            " 7 6 3 5 5 5 4 5 5 5 5 5 6 3 4 5 5 5 7 5 5 5 5 0 3 6 5 4 5 6 5 5 5 5 5 3 5\n",
            " 6 5 5 3 1 5 5 5 5 6 3 5 5 1 4 5 5 1 3 5 5 1 4 6 4 3 5 5 5 7 5 1 5 1 7 5 5\n",
            " 5 1 7 5 5 4 5 5 5 6 3 5 6 1 3 5 5 3 5 4 5 4 5 4 5 5 5 0 5 5 4 1 5 4 5 5 5\n",
            " 5 5 5 5 4 5 5 5 5 6 5 5 5 5 1 6 6 5 3 5 5 5 3 5 3 0 5 5 5 6 6 1 5 3 5 1 5\n",
            " 5 7 5 5 5 5 5 5 5 4 5 5 4 1 5 1 1 5 5 5 5 5 4 5 6 5 6 1 7 5 5 5 5 5 6 6 6\n",
            " 6 5 6 6 7 5 6 3 5 5 5 5 5 5 5 3 5 5 1 5 4 4 5 5 5 5 6 5 5 5 0 1 4 5 6 5 3\n",
            " 5 5 1 2 5 6 1 5 4 6 1 0 4 5 5 5 5 5 5 5 5 6 6 6 5 5 6 4 5 5 5 5 4 5 5 3 4\n",
            " 5 3 3 5 5 6 5 5 5 6 6 7 1 5 5 1 5 5 5 7 5 6 5 5 5 6 3 5 5 5 5 3 5 5 5 5 5\n",
            " 6 4 5 5 5 4 5 7 5 5 3 1 6 5 5 5 5 5 3 6 5 5 5 5 5 6 5 6 7 3 4 5 5 5 7 5 5\n",
            " 6 5 5 3 5 1 5 5 4 3 5 1 4 3 5 3 5 5 1 6 7 5 5 6 4 3 5 3 2 5 5 5 5 5 5 6 3\n",
            " 4 4 6 5 4 5 5 5 5 5 0 5 5 3 5 5 4 5 5 5 3 6 5 7 5 5 6 7 5 6 6 6 5 5 5 6 6\n",
            " 1 5 5 1 4 5 6 5 7 5 5 5 5 5 7 5 5 6 5 5 1 7 4 5 5 1 6 3 5 5 5 6 1 5 5 3 5\n",
            " 5 1 5 5 5 1 5 3 5 5 4 4 1 7 5 5 5 5 5 5 5 4 5 5 7 4 5 5 6 5 3 1 5 6 5 4 4\n",
            " 5 5 3 5 6 5 3 1 5 5 7 3 4 5 2 5 4 6 6 5 5 5 5 4 5 5 6 5 5 5 5 5 1 5 5 6 5\n",
            " 3 5 5 7 3 5 1 4 1 5 5 7 7 5 5 5 5 5 4 5 5 5 5 5 6 5 5 6 6 5 6 5 5 5 3 6 5\n",
            " 5 5 3 5 5 4 5 5 1 5 4 5 5 1 4 3 5 7 0 5 5 3 5 5 5 6 5 4 4 5 7 5 7 5 5 5 5\n",
            " 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#[CLS] Token Representation\n",
        "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
        "\n",
        "# Calculate balanced accuracy for training data\n",
        "balanced_acc_train = balanced_accuracy_score(y_train_original, y_pred_train)\n",
        "\n",
        "# Calculate balanced accuracy for validation data\n",
        "balanced_acc_val = balanced_accuracy_score(y_val_original, y_pred_val)\n",
        "\n",
        "# Print the balanced accuracy\n",
        "print(\"Training - Balanced Accuracy:\", balanced_acc_train)\n",
        "print(\"Validation - Balanced Accuracy:\", balanced_acc_val)\n",
        "\n",
        "# Print classification report for training data\n",
        "print(\"Training Classification Report:\")\n",
        "print(classification_report(y_train_original, y_pred_train))\n",
        "\n",
        "# Print classification report for validation data\n",
        "print(\"Validation Classification Report:\")\n",
        "print(classification_report(y_val_original, y_pred_val))\n",
        "\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBl2KNjtBJux",
        "outputId": "6d01a2b0-9d54-4708-9fe1-239562916c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training - Balanced Accuracy: 0.9812325483578208\n",
            "Validation - Balanced Accuracy: 0.6220487864101887\n",
            "Training Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      1.00      0.97       254\n",
            "           1       0.84      0.98      0.90       307\n",
            "           2       1.00      1.00      1.00       156\n",
            "           3       0.92      0.99      0.95       490\n",
            "           4       0.94      0.99      0.96       467\n",
            "           5       1.00      0.92      0.96      2385\n",
            "           6       0.92      0.98      0.95       716\n",
            "           7       0.94      1.00      0.97       360\n",
            "\n",
            "    accuracy                           0.96      5135\n",
            "   macro avg       0.94      0.98      0.96      5135\n",
            "weighted avg       0.96      0.96      0.96      5135\n",
            "\n",
            "Validation Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.31      0.29        13\n",
            "           1       0.55      0.78      0.65        59\n",
            "           2       0.67      0.50      0.57         4\n",
            "           3       0.56      0.54      0.55        81\n",
            "           4       0.69      0.85      0.77        96\n",
            "           5       0.88      0.76      0.82       586\n",
            "           6       0.54      0.72      0.62       120\n",
            "           7       0.58      0.51      0.55        41\n",
            "\n",
            "    accuracy                           0.73      1000\n",
            "   macro avg       0.59      0.62      0.60      1000\n",
            "weighted avg       0.76      0.73      0.74      1000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uoj4tsgAFBsx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "mGsToSckA4s8",
        "JrBb7otPBHuX"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0058f937e6bb4036a0ea0966493e9df9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce47413d0c0d48c1a7f2f684537141f0",
              "IPY_MODEL_e9d44197edd243ef8347b7a4155e746d",
              "IPY_MODEL_9728fbbe4381424f844fc58f3a9e3300"
            ],
            "layout": "IPY_MODEL_7e58b30924ff4cf0a84843658747b2c4"
          }
        },
        "ce47413d0c0d48c1a7f2f684537141f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_116afa98800d47b6ac34230a2a049191",
            "placeholder": "​",
            "style": "IPY_MODEL_1888873906d7490aa6ed550bf63db59d",
            "value": "tf_model.h5: 100%"
          }
        },
        "e9d44197edd243ef8347b7a4155e746d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c5d795f54524de98b940b07277c429f",
            "max": 1472567624,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_497bd187bc994f24bc9363cdf55da80f",
            "value": 1472567624
          }
        },
        "9728fbbe4381424f844fc58f3a9e3300": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54d376ed9c06439cb8f6d2c4d91cb3ea",
            "placeholder": "​",
            "style": "IPY_MODEL_9b5f190d12a94be085953c89c5d93337",
            "value": " 1.47G/1.47G [00:04&lt;00:00, 323MB/s]"
          }
        },
        "7e58b30924ff4cf0a84843658747b2c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "116afa98800d47b6ac34230a2a049191": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1888873906d7490aa6ed550bf63db59d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c5d795f54524de98b940b07277c429f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "497bd187bc994f24bc9363cdf55da80f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54d376ed9c06439cb8f6d2c4d91cb3ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b5f190d12a94be085953c89c5d93337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}